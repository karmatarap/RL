{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-armed bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected reward given that action a is selected: $q_*(a) = E[R_t|A_t=a]$. \n",
    "The estimated value of action a at timestep t: $Q_t(a)$.\n",
    "We want $Q_t(a)$ to be as close to $q_*(a)$ as possible.\n",
    "\n",
    "For actions that follow a distribution, the true value of an action is the mean reward for that action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_*(a) = E[R_t|A_t=a] = \\sum_r{p(r|a) r}$ for discrete values\n",
    "Integrals for continuous\n",
    "\n",
    "\n",
    "Our estimate $Q_t = \\frac{\\sum_{i=1}^{t-1}R_i}{t-1}$\n",
    "\n",
    "Here we are basing $Q_t$ time t-1 as Reward will be from that timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action - value method\n",
    "\n",
    "$Q_t(a) = \\frac{\\text{sum of rewards when a taken prior to t}}{\\text{number of times a taken prior to t }} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy estimate of action\n",
    "\n",
    "$A_t=argmax_a Q_t(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental update\n",
    "$\\text{New Estimate}\\leftarrow\\text{Old Estimate} + \\text{Step Size [Target - Old Estimate]}$ \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken:  0 Reward:  1.219302863871272\n",
      "Action taken:  0 Reward:  -3.4662487364323376\n",
      "Action taken:  1 Reward:  -0.5173949640217113\n",
      "Action taken:  1 Reward:  3.2843830914315104\n",
      "Action taken:  1 Reward:  2.7991695889607344\n",
      "Action taken:  1 Reward:  2.057964710092238\n",
      "Action taken:  1 Reward:  -0.6945817891563733\n",
      "Action taken:  1 Reward:  0.7228260095526183\n",
      "Action taken:  1 Reward:  -1.534522317128514\n",
      "Action taken:  1 Reward:  1.3072859745825902\n",
      "Action taken:  1 Reward:  -0.6050778500792697\n",
      "Action taken:  1 Reward:  1.8016258605481874\n",
      "Action taken:  1 Reward:  2.5667613402513325\n",
      "Action taken:  1 Reward:  0.7660940051634154\n",
      "Action taken:  1 Reward:  -0.8661311194921166\n",
      "Action taken:  1 Reward:  0.4345054505457312\n",
      "Action taken:  1 Reward:  1.6780679978541009\n",
      "Action taken:  1 Reward:  -1.372085066568621\n",
      "Action taken:  1 Reward:  1.9841120510319377\n",
      "Action taken:  1 Reward:  5.030377641921631\n",
      "Action taken:  1 Reward:  2.81981477428809\n",
      "Action taken:  1 Reward:  -0.7061061828426729\n",
      "Action taken:  1 Reward:  -1.185675408790372\n",
      "Action taken:  1 Reward:  -2.1922449888818827\n",
      "Action taken:  1 Reward:  -1.6338969309885254\n",
      "Action taken:  1 Reward:  3.8980360159772287\n",
      "Action taken:  1 Reward:  4.513719398068195\n",
      "Action taken:  1 Reward:  2.268540398410072\n",
      "Action taken:  1 Reward:  2.403797742411294\n",
      "Action taken:  1 Reward:  3.621965816630515\n",
      "Action taken:  1 Reward:  -1.9219100853060098\n",
      "Action taken:  1 Reward:  -4.577541399185464\n",
      "Action taken:  1 Reward:  1.9148453823550504\n",
      "Action taken:  1 Reward:  3.4316850772035443\n",
      "Action taken:  1 Reward:  2.3368723734650185\n",
      "Action taken:  1 Reward:  -2.2783521283743227\n",
      "Action taken:  1 Reward:  3.0286587287576054\n",
      "Action taken:  1 Reward:  2.5742347066061377\n",
      "Action taken:  1 Reward:  -0.23312297942180593\n",
      "Action taken:  1 Reward:  -0.8426748238670909\n",
      "Action taken:  1 Reward:  2.04406932722448\n",
      "Action taken:  1 Reward:  0.4461852214780143\n",
      "Action taken:  1 Reward:  0.45925983513219615\n",
      "Action taken:  1 Reward:  0.1619158530821555\n",
      "Action taken:  1 Reward:  3.4376799017371154\n",
      "Action taken:  1 Reward:  0.04800652560688412\n",
      "Action taken:  1 Reward:  -2.494126893299901\n",
      "Action taken:  1 Reward:  -3.2906817569794446\n",
      "Action taken:  1 Reward:  -3.9702009250708006\n",
      "Action taken:  1 Reward:  1.7738930155934762\n",
      "Action taken:  2 Reward:  1\n",
      "Action taken:  1 Reward:  -1.431567367249282\n",
      "Action taken:  1 Reward:  -2.18865747154658\n",
      "Action taken:  1 Reward:  5.388426865539461\n",
      "Action taken:  1 Reward:  0.6389996896658747\n",
      "Action taken:  1 Reward:  5.168784419919348\n",
      "Action taken:  1 Reward:  1.3242353968535052\n",
      "Action taken:  1 Reward:  0.6278362201427743\n",
      "Action taken:  1 Reward:  -2.5434726860083563\n",
      "Action taken:  1 Reward:  1.3516837500691676\n",
      "Action taken:  2 Reward:  1\n",
      "Action taken:  1 Reward:  -0.2595771157838942\n",
      "Action taken:  1 Reward:  2.165659073559971\n",
      "Action taken:  1 Reward:  -0.7940092688980549\n",
      "Action taken:  1 Reward:  -1.6542567493646456\n",
      "Action taken:  1 Reward:  4.386608852186196\n",
      "Action taken:  1 Reward:  0.7838491104154027\n",
      "Action taken:  1 Reward:  -0.2019450389903794\n",
      "Action taken:  1 Reward:  2.7767297233642187\n",
      "Action taken:  1 Reward:  1.5536738448532788\n",
      "Action taken:  1 Reward:  1.053999951955414\n",
      "Action taken:  1 Reward:  1.5578675900572487\n",
      "Action taken:  1 Reward:  -0.4870491007277973\n",
      "Action taken:  0 Reward:  2.812598639710793\n",
      "Action taken:  1 Reward:  0.7013785321333822\n",
      "Action taken:  1 Reward:  -0.6205446776321482\n",
      "Action taken:  1 Reward:  1.8944108462649436\n",
      "Action taken:  1 Reward:  0.8007493372226187\n",
      "Action taken:  1 Reward:  0.342908978310832\n",
      "Action taken:  1 Reward:  -2.1349324997825074\n",
      "Action taken:  0 Reward:  -2.660467672711769\n",
      "Action taken:  1 Reward:  1.5697954966234946\n",
      "Action taken:  1 Reward:  -0.3045341652876399\n",
      "Action taken:  1 Reward:  2.7170428244621423\n",
      "Action taken:  1 Reward:  1.3703054463430926\n",
      "Action taken:  1 Reward:  -1.3199292670829368\n",
      "Action taken:  1 Reward:  1.7489500614228226\n",
      "Action taken:  1 Reward:  2.6521497652961887\n",
      "Action taken:  1 Reward:  -1.985984647848814\n",
      "Action taken:  1 Reward:  2.873660668802999\n",
      "Action taken:  1 Reward:  -3.5834352184302585\n",
      "Action taken:  1 Reward:  3.339369017334308\n",
      "Action taken:  1 Reward:  -1.6002681568751993\n",
      "Action taken:  1 Reward:  -1.7376622084902582\n",
      "Action taken:  0 Reward:  0.5492262230970435\n",
      "Action taken:  1 Reward:  0.4971817898053519\n",
      "Action taken:  1 Reward:  3.4430083544658507\n",
      "Action taken:  1 Reward:  3.6797529104542948\n",
      "Action taken:  1 Reward:  3.4058844218424995\n",
      "Action taken:  1 Reward:  1.3004022385872267\n",
      "[-0.22973058  0.99486323  0.50149254]\n"
     ]
    }
   ],
   "source": [
    "#stationary problem: ie time does not affect the reward distribution\n",
    "\n",
    "def bandit(action):\n",
    "    \"\"\"\n",
    "    Our bandit will sample from 3 distributions\n",
    "    depending on action\n",
    "    \"\"\"\n",
    "    samples = [np.random.normal(0, 3),\n",
    "               np.random.normal(1, 2),\n",
    "               np.random.binomial(1, 0.5)]\n",
    "    return samples[action]\n",
    "\n",
    "\n",
    "# Simple bandit algorithm from Sutton et al.\n",
    "\n",
    "# Estimates of expected returns\n",
    "Q = np.zeros(3)\n",
    "\n",
    "# Number of times action a was taken\n",
    "N = np.zeros(3)\n",
    "\n",
    "# Explore 10 % of the time, be greedy 90% \n",
    "epsilon = 0.1\n",
    "for i in range(10000):\n",
    "    A = np.argmax(Q) if random.random() > epsilon else random.randint(0,2)\n",
    "    R = bandit(A)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Action taken: \", A, \"Reward: \", R)\n",
    "     \n",
    "    #Increment denominator\n",
    "    N[A] += 1\n",
    "    \n",
    "    #Update average reward of action A\n",
    "    # Q(A) = Q(A) + 1/n * [R - Q(A)], for sample average, step size = 1/n\n",
    "    Q[A] += (R-Q[A])/N[A]\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-stationary problem\n",
    "\n",
    "If time affects the distribution of the rewards, we can use a fixed step size, this weights recent rewards more heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values\n",
    "\n",
    "Encourages exploration at the start, however can cause problems with non-stationary problems. Often we dont know what an optimistic value is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken:  0 Reward:  -1.90661875904948\n",
      "Action taken:  2 Reward:  1\n",
      "Action taken:  1 Reward:  2.0233167441441884\n",
      "Action taken:  1 Reward:  2.051044473220472\n",
      "Action taken:  1 Reward:  2.4008229062289335\n",
      "Action taken:  1 Reward:  -2.966628415982666\n",
      "Action taken:  1 Reward:  1.6921276872579605\n",
      "Action taken:  1 Reward:  -1.7393040385207494\n",
      "Action taken:  1 Reward:  3.6389524595195444\n",
      "Action taken:  0 Reward:  -4.456834828382192\n",
      "Action taken:  1 Reward:  0.7390826604431331\n",
      "Action taken:  1 Reward:  -0.8772018830684871\n",
      "Action taken:  1 Reward:  2.8448751278749342\n",
      "Action taken:  1 Reward:  1.233988647681002\n",
      "Action taken:  1 Reward:  3.9130542487885793\n",
      "Action taken:  1 Reward:  -3.5142072238911215\n",
      "Action taken:  1 Reward:  1.7099071849030385\n",
      "Action taken:  1 Reward:  0.8486734441944267\n",
      "Action taken:  1 Reward:  2.604347360146787\n",
      "Action taken:  1 Reward:  -0.44959265286405015\n",
      "Action taken:  1 Reward:  -0.07331565787503624\n",
      "Action taken:  1 Reward:  0.039906544562063107\n",
      "Action taken:  1 Reward:  0.5135452263492137\n",
      "Action taken:  1 Reward:  2.880227694667749\n",
      "Action taken:  1 Reward:  3.525513741641677\n",
      "Action taken:  1 Reward:  1.888131220163602\n",
      "Action taken:  1 Reward:  1.595415889476966\n",
      "Action taken:  1 Reward:  1.2150363541542053\n",
      "Action taken:  1 Reward:  -2.9382056996276154\n",
      "Action taken:  1 Reward:  -2.629367617479114\n",
      "Action taken:  1 Reward:  2.573499011564239\n",
      "Action taken:  1 Reward:  -1.0580809027522342\n",
      "Action taken:  1 Reward:  -1.2708521399654225\n",
      "Action taken:  1 Reward:  -0.7332655443408396\n",
      "Action taken:  0 Reward:  -1.4832927551108013\n",
      "Action taken:  1 Reward:  1.94018218916375\n",
      "Action taken:  1 Reward:  -0.23162069292124188\n",
      "Action taken:  1 Reward:  2.3231376739920275\n",
      "Action taken:  1 Reward:  3.1533535386821465\n",
      "Action taken:  1 Reward:  0.07112461494084354\n",
      "Action taken:  1 Reward:  -2.641834448725923\n",
      "Action taken:  1 Reward:  3.0369023755302336\n",
      "Action taken:  1 Reward:  6.471411757997969\n",
      "Action taken:  1 Reward:  -1.2404166568092796\n",
      "Action taken:  1 Reward:  -0.5096504149148333\n",
      "Action taken:  1 Reward:  1.401855020366616\n",
      "Action taken:  1 Reward:  -1.0264335471792876\n",
      "Action taken:  1 Reward:  -6.060923620830885\n",
      "Action taken:  1 Reward:  0.000910138960195428\n",
      "Action taken:  1 Reward:  0.03727768811197574\n",
      "Action taken:  1 Reward:  0.20262247966019997\n",
      "Action taken:  1 Reward:  4.548762928569788\n",
      "Action taken:  1 Reward:  2.26524561456769\n",
      "Action taken:  1 Reward:  5.264870290803535\n",
      "Action taken:  1 Reward:  3.195222938811107\n",
      "Action taken:  1 Reward:  -0.13736936610142547\n",
      "Action taken:  1 Reward:  0.947714343771707\n",
      "Action taken:  1 Reward:  -1.0388078061749622\n",
      "Action taken:  1 Reward:  0.3366392610418164\n",
      "Action taken:  2 Reward:  0\n",
      "Action taken:  1 Reward:  -1.058691240776949\n",
      "Action taken:  1 Reward:  0.9308429139912355\n",
      "Action taken:  1 Reward:  0.4742640423185148\n",
      "Action taken:  2 Reward:  1\n",
      "Action taken:  1 Reward:  1.641520685382807\n",
      "Action taken:  1 Reward:  3.999512299787384\n",
      "Action taken:  1 Reward:  2.337929105599043\n",
      "Action taken:  1 Reward:  1.7314348320359454\n",
      "Action taken:  1 Reward:  1.5385598742315985\n",
      "Action taken:  1 Reward:  -1.5777616205123182\n",
      "Action taken:  0 Reward:  -0.35559719358498104\n",
      "Action taken:  0 Reward:  -0.505232152136456\n",
      "Action taken:  1 Reward:  2.169162035467944\n",
      "Action taken:  1 Reward:  -1.5174844561177854\n",
      "Action taken:  1 Reward:  1.0341398473513994\n",
      "Action taken:  1 Reward:  2.019990989372264\n",
      "Action taken:  0 Reward:  1.8677778897613921\n",
      "Action taken:  1 Reward:  -0.9659844803952751\n",
      "Action taken:  1 Reward:  0.7266314426277145\n",
      "Action taken:  1 Reward:  -0.9540959575420138\n",
      "Action taken:  1 Reward:  5.151783895546942\n",
      "Action taken:  1 Reward:  0.27707479920050615\n",
      "Action taken:  1 Reward:  1.9148830778166632\n",
      "Action taken:  1 Reward:  1.3676393594564265\n",
      "Action taken:  1 Reward:  3.7328040917602734\n",
      "Action taken:  1 Reward:  1.1606408207287173\n",
      "Action taken:  1 Reward:  2.5796290795945405\n",
      "Action taken:  1 Reward:  -0.9990035115077367\n",
      "Action taken:  1 Reward:  0.9515956227649321\n",
      "Action taken:  1 Reward:  2.79135314354402\n",
      "Action taken:  1 Reward:  1.160064656558847\n",
      "Action taken:  1 Reward:  1.4184175184464967\n",
      "Action taken:  1 Reward:  -0.4572603886583577\n",
      "Action taken:  1 Reward:  -0.6057967319530586\n",
      "Action taken:  1 Reward:  -0.06256305530565331\n",
      "Action taken:  1 Reward:  1.268600861770515\n",
      "Action taken:  1 Reward:  0.7479624752566815\n",
      "Action taken:  1 Reward:  -2.4687817758486568\n",
      "Action taken:  1 Reward:  0.5174572188039812\n",
      "Action taken:  1 Reward:  1.221532089100879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.13302256,  0.98904705,  0.48723404])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def epsilon_bandit(Q=np.zeros(3), e=0.1, n=1000):\n",
    "    d = len(Q)\n",
    "    N = np.zeros(d)\n",
    "\n",
    "    for i in range(10000):\n",
    "        A = np.argmax(Q) if random.random() > e else np.random.choice(d)\n",
    "        R = bandit(A)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Action taken: \", A, \"Reward: \", R)\n",
    "\n",
    "        #Increment denominator\n",
    "        N[A] += 1\n",
    "\n",
    "        #Update average reward of action A\n",
    "        # Q(A) = Q(A) + 1/n * [R - Q(A)], for sample average, step size = 1/n\n",
    "        Q[A] += (R-Q[A])/N[A]\n",
    "        \n",
    "    return Q\n",
    "\n",
    "q = epsilon_bandit(Q=np.array([2.,2.,2.]))\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the confidence interval from our samples, then pick the action with the highest Upper Confidence Interval\n",
    "\n",
    "$A_t=argmax[Q_t(a) + c\\sqrt{\\frac{log t}{N_t(a)}}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken:  1 Reward:  2.926303218235967\n",
      "Action taken:  1 Reward:  -0.6782447838138157\n",
      "Action taken:  1 Reward:  0.4006409129036955\n",
      "Action taken:  1 Reward:  2.204931908553979\n",
      "Action taken:  1 Reward:  0.7932834958605895\n",
      "Action taken:  1 Reward:  0.9738712151018813\n",
      "Action taken:  1 Reward:  1.969108856151872\n",
      "Action taken:  1 Reward:  1.2122579993527216\n",
      "Action taken:  1 Reward:  5.038366810074165\n",
      "Action taken:  1 Reward:  -0.9071635456073599\n",
      "Action taken:  1 Reward:  2.891078106598414\n",
      "Action taken:  1 Reward:  -1.5544866365924364\n",
      "Action taken:  1 Reward:  1.7421427717961158\n",
      "Action taken:  1 Reward:  0.9033339659020148\n",
      "Action taken:  1 Reward:  3.452605637345256\n",
      "Action taken:  1 Reward:  0.04736580165337634\n",
      "Action taken:  2 Reward:  1\n",
      "Action taken:  1 Reward:  -0.7639604681841909\n",
      "Action taken:  1 Reward:  0.5851406076081012\n",
      "Action taken:  1 Reward:  4.678176925026728\n",
      "Action taken:  1 Reward:  1.3421557894835343\n",
      "Action taken:  1 Reward:  -0.5358740714164438\n",
      "Action taken:  1 Reward:  2.35165345155716\n",
      "Action taken:  1 Reward:  -0.9218793878901583\n",
      "Action taken:  1 Reward:  -2.3749610203138767\n",
      "Action taken:  1 Reward:  0.7836053894189489\n",
      "Action taken:  1 Reward:  4.990199928046275\n",
      "Action taken:  1 Reward:  1.3561835599671799\n",
      "Action taken:  1 Reward:  -0.8864523586070985\n",
      "Action taken:  1 Reward:  2.018743729389416\n",
      "Action taken:  1 Reward:  1.9813078857653332\n",
      "Action taken:  1 Reward:  0.6316913701565456\n",
      "Action taken:  1 Reward:  2.123357146407549\n",
      "Action taken:  1 Reward:  1.0224542326058388\n",
      "Action taken:  1 Reward:  -1.5908219914307908\n",
      "Action taken:  1 Reward:  1.4185860954349934\n",
      "Action taken:  1 Reward:  3.822021277419139\n",
      "Action taken:  1 Reward:  1.5600603043385113\n",
      "Action taken:  1 Reward:  0.7098588934883074\n",
      "Action taken:  1 Reward:  4.277881701950486\n",
      "Action taken:  1 Reward:  -1.6020402716985918\n",
      "Action taken:  1 Reward:  -2.6195838960516733\n",
      "Action taken:  1 Reward:  2.605386275747181\n",
      "Action taken:  1 Reward:  1.5266938321333352\n",
      "Action taken:  1 Reward:  3.715551316740592\n",
      "Action taken:  1 Reward:  3.496513204845829\n",
      "Action taken:  1 Reward:  1.1416380757328715\n",
      "Action taken:  1 Reward:  1.376954430503441\n",
      "Action taken:  1 Reward:  -0.14033859467170662\n",
      "Action taken:  1 Reward:  -0.010190196270777774\n",
      "Action taken:  1 Reward:  3.1298594004076157\n",
      "Action taken:  1 Reward:  0.4210866268156571\n",
      "Action taken:  1 Reward:  0.252185911559215\n",
      "Action taken:  1 Reward:  2.6474632309082544\n",
      "Action taken:  1 Reward:  -4.651723303082993\n",
      "Action taken:  1 Reward:  2.34758162843029\n",
      "Action taken:  1 Reward:  3.194078751812552\n",
      "Action taken:  1 Reward:  3.3272402465314492\n",
      "Action taken:  1 Reward:  -0.8749035901205715\n",
      "Action taken:  1 Reward:  0.846523478601672\n",
      "Action taken:  1 Reward:  3.1393068443517245\n",
      "Action taken:  1 Reward:  -0.6745092572157141\n",
      "Action taken:  1 Reward:  0.6521968044500066\n",
      "Action taken:  1 Reward:  1.7722237741537672\n",
      "Action taken:  1 Reward:  1.7463577805352763\n",
      "Action taken:  1 Reward:  5.8746932998759736\n",
      "Action taken:  1 Reward:  3.393958442515652\n",
      "Action taken:  1 Reward:  -0.39226061563342407\n",
      "Action taken:  1 Reward:  6.536785789403526\n",
      "Action taken:  1 Reward:  -1.196528405504445\n",
      "Action taken:  1 Reward:  -0.9327487171735072\n",
      "Action taken:  1 Reward:  2.408103694034868\n",
      "Action taken:  1 Reward:  0.0006499497712790792\n",
      "Action taken:  1 Reward:  0.6905097789850267\n",
      "Action taken:  1 Reward:  3.962610464272829\n",
      "Action taken:  1 Reward:  1.5154181858369302\n",
      "Action taken:  1 Reward:  -1.0512055269411977\n",
      "Action taken:  1 Reward:  -1.9066467904795812\n",
      "Action taken:  1 Reward:  0.06458357417208638\n",
      "Action taken:  1 Reward:  2.078952493717355\n",
      "Action taken:  1 Reward:  3.869231737510378\n",
      "Action taken:  1 Reward:  0.23475736257496527\n",
      "Action taken:  1 Reward:  3.0641927323202016\n",
      "Action taken:  1 Reward:  0.2519173083619547\n",
      "Action taken:  1 Reward:  0.07170569232452884\n",
      "Action taken:  1 Reward:  -2.140830594464365\n",
      "Action taken:  1 Reward:  3.4711787214023637\n",
      "Action taken:  1 Reward:  1.6214385163964087\n",
      "Action taken:  1 Reward:  1.607467439982067\n",
      "Action taken:  1 Reward:  1.1719721554103322\n",
      "Action taken:  1 Reward:  0.34727795008822504\n",
      "Action taken:  1 Reward:  3.9892841040353657\n",
      "Action taken:  1 Reward:  3.03850432666416\n",
      "Action taken:  1 Reward:  5.0067613500182055\n",
      "Action taken:  1 Reward:  1.8145839687811782\n",
      "Action taken:  1 Reward:  2.1051074213532583\n",
      "Action taken:  1 Reward:  1.4165394243952063\n",
      "Action taken:  1 Reward:  -2.079931114794287\n",
      "Action taken:  1 Reward:  2.669552040563362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4.13603749,  1.0173002 ,  0.48148148])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ucb(Q=np.zeros(3),c=2, n=1000):\n",
    "    d = len(Q)\n",
    "    N = np.zeros(d)\n",
    "    \n",
    "    def uci(a, t):\n",
    "        return Q[a] + c * math.sqrt(math.log(t)/(N[a]+1))\n",
    "\n",
    "    for i in range(1,10000):\n",
    "        x = np.array([uci(a, i) for a in range(3)])\n",
    "        A = np.argmax(x)  \n",
    "        R = bandit(A)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Action taken: \", A, \"Reward: \", R)\n",
    "\n",
    "        #Increment denominator\n",
    "        N[A] += 1\n",
    "\n",
    "        #Update average reward of action A\n",
    "        # Q(A) = Q(A) + 1/n * [R - Q(A)], for sample average, step size = 1/n\n",
    "        Q[A] += (R-Q[A])/N[A]\n",
    "        \n",
    "    return Q\n",
    "\n",
    "q = ucb()\n",
    "q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
