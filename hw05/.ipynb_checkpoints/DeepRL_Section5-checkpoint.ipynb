{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI E-89C Deep Reinforcement Learning, Spring 2020\n",
    "### Section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-Visit Monte Carlo (MC) prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider Environment that has five states: 1, 2, 3, 4, and 5. Possible transitions are: (1) 1->1, 1->2; (2) 2->1, 2->2, 2->3; (3) 3->2, 3->3, 3->4; (4) 4->3, 4->4, 4->5; (5) 5->4, 5->5.\n",
    "\n",
    "Actions of the Agent are decoded by -1, 0, and +1, which correspond to its intention to move left, stay, and move right, respectively. The Environment, however, does not always respond to these intentions exactly, and there is 10% chance that action 0 will result in moving to the left (if moving to the left is admissible), and +1 action will result in staying - in other words, there is an \"east wind.\" More specifically, the non-zero transition probabilities $p(s^\\prime,r|s,a)$ are<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=1,a=0)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=1,a=+1)=0.1,p(s^\\prime=2,r=0|s=1,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=2,a=-1)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=2,a=0)=0.1,p(s^\\prime=2,r=0|s=2,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=2,r=0|s=2,a=+1)=0.1,p(s^\\prime=3,r=1|s=2,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=2,r=0|s=3,a=-1)=1$,<br>\n",
    "$p(s^\\prime=2,r=0|s=3,a=0)=0.1,p(s^\\prime=3,r=1|s=3,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=3,r=1|s=3,a=+1)=0.1,p(s^\\prime=4,r=0|s=3,a=+1)=0.9$,<br>\n",
    "\n",
    "etc.\n",
    "\n",
    "Further, we assume that whenever the process enters state 3, the Environment generates reward = 1. In all other cases the reward is 0. For example, transition 2->3 will result in reward 1, transition 3->3 will result in reward 1, transition 3->2 will result in reward 0, transition 2->2 will result in reward 0, etc.\n",
    "\n",
    "\n",
    "\n",
    "Further, assume that the agent does not know about the wind or what rewards to expect. It chooses to stay in all states, i.e. the policy is\n",
    "$\\pi(-1|1)=0, \\pi(0|1)=1, \\pi(+1|1)=0$,<br>\n",
    "$\\pi(-1|2)=0, \\pi(0|2)=1, \\pi(+1|2)=0$,<br>\n",
    "$\\pi(-1|3)=0, \\pi(0|3)=1, \\pi(+1|3)=0$,<br>\n",
    "$\\pi(-1|4)=0, \\pi(0|4)=1, \\pi(+1|4)=0$,<br>\n",
    "etc.\n",
    "\n",
    "Letâ€™s use $\\gamma=0.9$ and $T=100$. Action-value function via First-Visit Monte Carlo (MC) prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 5: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        if self.state > 1 and move > -1:\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, A0=0):\n",
    "        self.current_reward = 0.0\n",
    "        self.current_action = A0\n",
    "\n",
    "    def step(self, env):\n",
    "        #actions = env.admissible_actions()\n",
    "        action_selected = 0\n",
    "        if env.time == 0:\n",
    "            action_selected = self.current_action\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_action = action_selected \n",
    "        self.current_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_episode(S0, A0, T=10):\n",
    "    env = Environment(S0)\n",
    "    agent = Agent(A0)\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    for t in range(T+1):\n",
    "        states.append(env.state)\n",
    "        agent.step(env)\n",
    "        actions.append(agent.current_action)\n",
    "        rewards.append(agent.current_reward)\n",
    "    return [states, actions, rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 5, 5, 5, 5, 5, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_episode(4,1, T=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 2, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_episode(4,0, T=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_episode(4,-1, T=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 0, 1]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S0 = 2\n",
    "env = Environment(S0)\n",
    "env.admissible_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 -1\n",
      "2 1\n",
      "3 0\n",
      "4 1\n",
      "2 0\n",
      "4 0\n",
      "3 0\n",
      "5 0\n",
      "4 1\n",
      "4 0\n",
      "2 0\n",
      "5 0\n",
      "2 0\n",
      "2 -1\n",
      "4 -1\n",
      "4 0\n",
      "1 1\n",
      "4 1\n",
      "2 -1\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "T = 100\n",
    "N = np.zeros((5, 3))\n",
    "Q = np.zeros((5, 3))\n",
    "\n",
    "for k in range(4000):\n",
    "        S0 = np.random.randint(low=1, high=6, size=1)[0]\n",
    "        env = Environment(S0)\n",
    "        A0 = np.random.choice(env.admissible_actions())\n",
    "        if k < 20:\n",
    "            print(S0,A0)\n",
    "        episode = gen_episode(S0, A0, T)\n",
    "        G = 0\n",
    "        for t in reversed(range(0,T)):\n",
    "            S = episode[0][t]\n",
    "            A = episode[1][t]\n",
    "            R = episode[2][t]\n",
    "            G = gamma*G + R\n",
    "            if (S not in episode[0][0:t]) and (A not in episode[1][0:t]):\n",
    "                N[S-1,A+1] = N[S-1,A+1] + 1\n",
    "                Q[S-1,A+1] = Q[S-1,A+1] + 1/N[S-1,A+1]*(G - Q[S-1,A+1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 4.79],\n",
       "       [0.  , 4.8 , 2.72],\n",
       "       [5.38, 2.77, 1.24],\n",
       "       [2.56, 1.27, 0.  ]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
