{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Karma Tarap\n",
    "### CSCI E-89C Deep Reinforcement Learning  \n",
    "### Part II of Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environment below has three states: 1, 2, and 3. Possible transitions are: (1) 1->1, 1->2; (2) 2->1, 2->2, 2->3; and (3) 3->2, 3->3.\n",
    "\n",
    "Actions of the Agent are decoded by -1, 0, and +1, which correspond to its intention to move left, stay, and move right, respectively. The Environment, however, does not always respond to these intentions exactly, and there is 10% chance that action 0 will result in moving to the left (if moving to the left is admissible), and 1 action will result in staying - in other words, there is an \"east wind\" (please see get_reward() of the Environment). \n",
    "\n",
    "Further, we assume that the number of steps, T, is infinite and whenever the process enters state 3, the Environment generates reward = 1. In all other cases the reward is 0. For example, transition 2->3 will result in reward 1, transition 3->3 will result in reward 1, transition 3->2 will result in reward 0, transition 2->2 will result in reward 0, etc.\n",
    "\n",
    "Let’s use $\\gamma=0.9$. Currently, the Agent always selects action 0 (has an intention to stay). Please notice that without the wind, the state-values would be [0,0,1/0.1] for this policy. With the wind, the value of state 3, however, becomes only 4.74 because the transition 3->2 happens sooner or later with probability 1. The Agent does not make an attempt to return to state 3 – its intention is to stay.\n",
    "\n",
    "Function reward_cumulative(T=10, S0=1, gamma=1) returns the observed cumulative discounted reward for T number of steps if the process starts in state S0. Please notice that given $\\gamma=0.9$, T=100 does not make this estimate much different from infinite time because $\\gamma^T$ is of order $10^{-5}$.\n",
    "\n",
    "Function V_estimate(T=10, S0=1, gamma=0.9, n_trials=10) calls reward_cumulative() function n_trials number of times and then estimates the state-value based on these n_trials paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 3: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        if (self.state > 1 and move > -1) or (self.state == 1 and move > 0):\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        #actions = env.admissible_actions()\n",
    "        action_selected = 0\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward\n",
    "        \n",
    "def reward_cumulative(T=10, S0=1, gamma=1):\n",
    "    env = Environment(S0)\n",
    "    agent = Agent()\n",
    "    G = 0\n",
    "    while env.time < T:\n",
    "        agent.step(env)\n",
    "        G += gamma**(env.time-1)*agent.current_reward\n",
    "    return G\n",
    "\n",
    "def V_estimate(T=10, S0=1, gamma=0.9, n_trials=10):\n",
    "    V_estimate = 0\n",
    "    for i in range(1,n_trials+1):\n",
    "        #V_estimate = (V_estimate*(i-1) + reward_cumulative(T, S0, gamma))/i\n",
    "        V_estimate = V_estimate+(reward_cumulative(T, S0, gamma)-V_estimate)/i \n",
    "    return V_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "[0.  0.  4.7]\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "V = np.array([V_estimate(T, S0 = s, gamma=0.9, n_trials=10000) for s in range(1,4)])\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"state-value function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 1 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please add state 4 and 5 to the Environment. For entering states 4 and 5 assume no rewards. Please keep current reward from entering state 3 as is, i.e. 2->3, 3->3, 4->3 will result in reward=1. All other cases correspond to 0 reward.\n",
    "\n",
    "For current actions of the Agent, keep $\\gamma=0.9$ and estimate the state-values using V_estimate() function with T = 100 and n_trials=10000. Print the result for states 1, 2, 3, 4, 5. What value of state 4 do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "[0.   0.   4.71 2.77 1.31]\n",
      "state-value function at state = 4:\n",
      "2.7709510275714795\n"
     ]
    }
   ],
   "source": [
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 5: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        if (self.state > 1 and move > -1) or (self.state == 1  and move > 0):\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "    \n",
    "T = 100\n",
    "V = np.array([V_estimate(T, S0 = s, gamma=0.9, n_trials=10000) for s in range(1,6)])\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"state-value function:\")\n",
    "print(V)\n",
    "print(\"state-value function at state = 4:\")\n",
    "print(V[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Environment you developed in Problem 1, change the actions of the Agent to the optimal, that is, from states 1 and 2 it will want to move to right, stay in stay 3, and move to left from states 4 and 5.\n",
    "\n",
    "For these actions of the Agent and $\\gamma=0.9$, estimate the state-values using V_estimate() function with T = 100 and n_trials=10000. Print the result for states 1, 2, 3, 4, 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        if env.state < 3:\n",
    "            action_selected = 1\n",
    "        elif env.state > 3:\n",
    "            action_selected = -1\n",
    "        else:\n",
    "            action_selected = 0\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward\n",
    "        \n",
    "T = 100\n",
    "V = np.array([V_estimate(T, S0 = s, gamma=0.9, n_trials=10000) for s in range(1,6)])\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"state-value function:\")\n",
    "print(V)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (10 points)\n",
    "For the Environment and Agent you developed in Problem 2, please obtain the state-value for this policy in an alternative way without simulations by solving the Bellman equation (eq. 4.5) numerically: use the iterative policy evaluation algorithm on p.75 of \"Reinforcement Learning\" by Sutton and Barto.\n",
    "\n",
    "Please notice that for these actions the policy $\\pi(a|s)$ is<br>\n",
    "$\\pi(-1|1)=0, \\pi(0|1)=0, \\pi(+1|1)=1$,<br>\n",
    "$\\pi(-1|2)=0, \\pi(0|2)=0, \\pi(+1|2)=1$,<br>\n",
    "$\\pi(-1|3)=0, \\pi(0|3)=1, \\pi(+1|3)=0$,<br>\n",
    "$\\pi(-1|4)=1, \\pi(0|4)=0, \\pi(+1|4)=0$,<br>\n",
    "etc.\n",
    "\n",
    "\n",
    "The non-zero transition probabilities $p(s^\\prime,r|s,a)$ are<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=1,a=0)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=1,a=+1)=0.1,p(s^\\prime=2,r=0|s=1,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=2,a=-1)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=2,a=0)=0.1,p(s^\\prime=2,r=0|s=2,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=2,r=0|s=2,a=+1)=0.1,p(s^\\prime=3,r=1|s=2,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=2,r=0|s=3,a=-1)=1$,<br>\n",
    "$p(s^\\prime=2,r=0|s=3,a=0)=0.1,p(s^\\prime=3,r=1|s=3,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=3,r=1|s=3,a=+1)=0.1,p(s^\\prime=4,r=0|s=3,a=+1)=0.9$,<br>\n",
    "\n",
    "etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_cumulative(T=10, S0=1, gamma=1):\n",
    "    env = Environment(S0)\n",
    "    agent = Agent()\n",
    "    G = 0\n",
    "    while env.time < T:\n",
    "        agent.step(env)\n",
    "        G += gamma**(env.time-1)*agent.current_reward\n",
    "    return G\n",
    "\n",
    "def V_estimate(T=10, S0=1, gamma=0.9, n_trials=10):\n",
    "    V_estimate = 0\n",
    "    for i in range(1,n_trials+1):\n",
    "        #V_estimate = (V_estimate*(i-1) + reward_cumulative(T, S0, gamma))/i\n",
    "        V_estimate = V_estimate+(reward_cumulative(T, S0, gamma)-V_estimate)/i \n",
    "    return V_estimate"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
