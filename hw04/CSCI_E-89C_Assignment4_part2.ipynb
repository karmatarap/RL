{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Karma Tarap\n",
    "### CSCI E-89C Deep Reinforcement Learning  \n",
    "### Part II of Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider Environment that has five states: 1, 2, 3, 4, and 5. Possible transitions are: (1) 1->1, 1->2; (2) 2->1, 2->2, 2->3; (3) 3->2, 3->3, 3->4; (4) 4->3, 4->4, 4->5; (5) 5->4, 5->5.\n",
    "\n",
    "Actions of the Agent are decoded by -1, 0, and +1, which correspond to its intention to move left, stay, and move right, respectively. The Environment, however, does not always respond to these intentions exactly, and there is 10% chance that action 0 will result in moving to the left (if moving to the left is admissible), and +1 action will result in staying - in other words, there is an \"east wind.\" More specifically, the non-zero transition probabilities $p(s^\\prime,r|s,a)$ are<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=1,a=0)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=1,a=+1)=0.1,p(s^\\prime=2,r=0|s=1,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=2,a=-1)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=2,a=0)=0.1,p(s^\\prime=2,r=0|s=2,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=2,r=0|s=2,a=+1)=0.1,p(s^\\prime=3,r=1|s=2,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=2,r=0|s=3,a=-1)=1$,<br>\n",
    "$p(s^\\prime=2,r=0|s=3,a=0)=0.1,p(s^\\prime=3,r=1|s=3,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=3,r=1|s=3,a=+1)=0.1,p(s^\\prime=4,r=0|s=3,a=+1)=0.9$,<br>\n",
    "\n",
    "etc.\n",
    "\n",
    "Further, we assume that whenever the process enters state 3, the Environment generates reward = 1. In all other cases the reward is 0. For example, transition 2->3 will result in reward 1, transition 3->3 will result in reward 1, transition 3->2 will result in reward 0, transition 2->2 will result in reward 0, etc.\n",
    "\n",
    "\n",
    "\n",
    "Further, assume that the agent does not know about the wind or what rewards to expect. It chooses to stay in all states, i.e. the policy is\n",
    "$\\pi(-1|1)=0, \\pi(0|1)=1, \\pi(+1|1)=0$,<br>\n",
    "$\\pi(-1|2)=0, \\pi(0|2)=1, \\pi(+1|2)=0$,<br>\n",
    "$\\pi(-1|3)=0, \\pi(0|3)=1, \\pi(+1|3)=0$,<br>\n",
    "$\\pi(-1|4)=0, \\pi(0|4)=1, \\pi(+1|4)=0$,<br>\n",
    "etc.\n",
    "\n",
    "Please estimate the action-value function using First-Visit Monte Carlo (MC) prediction. Make sure each pair $(S_0,A_0)$ has non-zero probability of being selected. Letâ€™s use $\\gamma=0.9$ and run the episodes for $T=100$. Make some reasonable selection of tolerance $\\theta$.\n",
    "\n",
    "Based on the estimates of $q_\\pi(s,a)$, please suggest a better policy. Is it optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1, A0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "        self.action = A0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 5: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        if self.state > 1 and move > -1:\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        self.action = move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        #actions = env.admissible_actions()\n",
    "        action_selected = 0\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 8.83, 8.84, 8.69])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_episode(T, env, agent):\n",
    "    episode = []\n",
    "    for i in range(T):\n",
    "        agent.step(env)\n",
    "        episode.append((env.state, env.action, agent.current_reward))\n",
    "    return episode\n",
    "\n",
    "def first_visit_mc(episodes = 10000, gamma=0.9):\n",
    "    R = np.zeros(5)\n",
    "    N = np.zeros(5)\n",
    "    T = 100\n",
    "    for _ in range(episodes):\n",
    "        env = Environment(random.choice(range(1,6)), random.choice([-1,0,1]))\n",
    "        agent = Agent()\n",
    "        episode = generate_episode(T, env, agent)\n",
    "        \n",
    "        states = [e[0] for e in episode]\n",
    "        rewards = [e[2] for e in episode]\n",
    "\n",
    "        #print(\"states:\",states)\n",
    "        #print(\"rewards:\",rewards)\n",
    "        G = 0\n",
    "        for j in range(T-1,-1,-1):\n",
    "            state = states[j]\n",
    "            G += rewards[j]*gamma\n",
    "            if state not in states[0:j]:\n",
    "                R[state-1] += G\n",
    "                N[state-1] += 1\n",
    "\n",
    "    return R / N\n",
    "\n",
    "first_visit_mc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 8.95, 8.85, 8.81])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, the agent will obtain the optimal policy via MC control. Please run the On-policy first-visit MC control (for $\\varepsilon$-soft policies)  without Exploring Starts algorithm for estimating $\\pi_*$. Use same $\\gamma=0.9$ and $T=100$.\n",
    "\n",
    "Does the final policy appear to be optimal?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
