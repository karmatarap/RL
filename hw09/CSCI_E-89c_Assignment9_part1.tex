\documentclass[12pt]{letter}
\usepackage[english]{babel}
\usepackage{amsmath}
%\pagestyle{empty}
\textwidth=6.1in \textheight=24cm \voffset=-3.7cm
\oddsidemargin=3.6mm
\usepackage{graphicx}
\pagestyle{empty}

\usepackage{enumerate}
\usepackage{setspace}
\usepackage[table]{xcolor}
\usepackage{bbm}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}


\begin{document}
\begin{flushleft}
{\sc Name: Karma Tarap}\\
CSCI E-89c Deep Reinforcement Learning\\
Part I of Assignment 9\\
\end{flushleft}

Please consider a Markov Decision Process (MDP) with  $\mathcal{S}=\{s^{A},s^{B},s^{C}\}$.\medskip\\
Given a particular state $s\in \mathcal{S}$, the agent is allowed to either try staying there or switching to any of the other states. Let's denote an intention to move to state $s^A$ by $a^A$, to state $s^B$ by $a^B$, and to state $s^C$ by $a^C$. The agent does not know transition probabilities, including the distributions of rewards. There is, however, some evidence that the agent gets rewards only at the entrance to $s^{C}$; and transition MDP probabilities to/from $s^{A}$ appear to be same (or nearly same) as to/from $s^{B}$. 

Suppose the agent chooses policy $\pi(a^A|s)=0.05$, $\pi(a^B|s)=0.05$, $\pi(a^C|s)=0.90$ for all $s\in\{s^A,s^B,s^C\}$. Because of the apparent symmetry between $s^A$ and $s^B$, it makes sense to assume that $v_\pi(s^A) \approx v_\pi(s^B)$ and approximate the state-values as follows:
$$v_\pi(s)\approx\hat{v}(s,{\bf w}) = w_1\cdot \mathbbm{1}_{(s=s_A)} + w_1\cdot \mathbbm{1}_{(s=s_B)} + w_2\cdot \mathbbm{1}_{(s=s_C)}.$$ 

Please notice that $\hat{v}(s^A,{\bf w})=\hat{v}(s^B,{\bf w})$ for any choice of weights.

Assume that the agent runs the following algorithm with $\alpha=0.1$ and $m=2$ for estimating $v_\pi$:
\begin{equation*}
\begin{aligned}
{\bf w}_{k+1} = {\bf w}_k + \alpha {\color{red}\sum_{t=mk}^{m(k+1)-1}}\left[R_{t+1}+ \gamma \hat{v}(S_{t+1},{\bf w}_k)-\hat{v}(S_t,{\bf w}_k)\right] \nabla \hat{v}(S_t,{\bf w}_k), ~k=0,1,2,\ldots
\end{aligned}
\end{equation*}
This algorithm is a modification of the Semi-gradient 1-step Temporal-Difference (TD) with the model now being trained in mini-batches of size $m$. Please use $\gamma=0.9$ and zero weights for $k=0$.

If the agent observes the following sequence of states, actions, and rewards:
{\small
\begin{equation*}
\begin{aligned}
&S_0=s^A,A_0=a^C,R_1=20,\\
&S_1=s^C,A_1=a^B,R_2=0,\\
&S_2=s^B,A_2=a^C,R_3=20,\\
&S_3=s^C,A_3=a^C,R_4=20,\\
&S_4=s^C,A_4=a^B,R_5=20,\\
&S_5=s^C,A_5=a^C,R_5=0,\\
&S_6=s^B,
\end{aligned}
\end{equation*}
find (a) weights ${\bf w}_k$ and (b) corresponding approximations $\hat{v}(s,{\bf w}_k)$ for iteration step $k =1,2,3$. Specifically, please fill the tables in below:\medskip\\
SOLUTION:\vspace{-0.4cm}

Gradient is: 
$\nabla \hat{v}(S_t,{\bf w}_t)=(\mathbbm{1}_{(s=s_A)}+\mathbbm{1}_{(s=s_B)},\mathbbm{1}_{(s=s_C)})^T,$

\begin{enumerate}[(a)]
\item weights ${\bf w}_k=(w_{1,k},w_{2,k})^T$:
{\doublespacing
\begin{center}
\begin{tabular}{ |c|c|c|c|c|}
\hline
  & $k=0$ & $k=1$ & $k=2$ & $k=3$\\
	\hline
	$w_{1,k}$ & 0 & 2 & 3.8 & 3.8\\
	\hline
	$w_{2,k}$ & 0 & 0 & 0 & 2.0\\
\hline
\end{tabular}
\end{center}
}

\item approximations $\hat{v}(s,{\bf w}_k)$:
{\doublespacing
\begin{center}
\begin{tabular}{ |c|c|c|c|c| }
\hline
  & $k=0$ & $k=1$ & $k=2$ & $k=3$ \\
	\hline
	$\hat{v}(s^A,{\bf w}_k)$ & 0 & 2 & 3.8 & 3.8\\
	\hline
	$\hat{v}(s^B,{\bf w}_k)$ & 0 & 2 & 3.8 & 3.8\\
	\hline
	$\hat{v}(s^C,{\bf w}_k)$ & 0 & 0 & 0 & 2.0\\
\hline
\end{tabular}
\end{center}
}
\end{enumerate}







\end{document}
